
显存消耗主要由以下因素决定：

模型参数
优化器状态（训练时）
激活值（前向传播）
梯度（反向传播）
批处理大小
1. OLMo-60M 显存需求
模型参数估算
隐藏层维度(d_model): 512
层数(n_layers): 8
注意力头数(n_heads): 8
词表大小(vocab_size): 50280
MLP扩展比例(mlp_ratio): 8
主要参数量计算：

词嵌入层: 50280 × 512 ≈ 25.7M
每个Transformer层:
自注意力: 3 × 512 × 512 (QKV投影) + 512 × 512 (输出投影) ≈ 1.05M
MLP: 512 × (512 × 8) + (512 × 8) × 512 ≈ 4.2M
层归一化: 2 × 512 ≈ 1K
每层总计: ≈ 5.3M
所有Transformer层: 8 × 5.3M ≈ 42.4M
总参数量: ≈ 68.1M (接近官方的60M)
训练显存需求
参数(FP16): 68.1M × 2B = 136.2MB
优化器状态(FP32): 68.1M × 4B × 2 = 544.8MB (AdamW需要两个状态)
梯度(FP16): 68.1M × 2B = 136.2MB
激活值: 取决于序列长度和批量大小
序列长度: 4096
批量大小: 8
估计激活值: ≈ 1-2GB
OLMo-60M训练总显存: 约3-4GB

2. OLMo-700M 显存需求
模型参数估算
隐藏层维度(d_model): 1536
层数(n_layers): 16
注意力头数(n_heads): 16
词表大小(vocab_size): 50280
MLP扩展比例(mlp_ratio): 8
主要参数量计算：

词嵌入层: 50280 × 1536 ≈ 77.2M
每个Transformer层:
自注意力: 3 × 1536 × 1536 + 1536 × 1536 ≈ 9.4M
MLP: 1536 × (1536 × 8) + (1536 × 8) × 1536 ≈ 37.7M
层归一化: 2 × 1536 ≈ 3K
每层总计: ≈ 47.1M
所有Transformer层: 16 × 47.1M ≈ 753.6M
总参数量: ≈ 830.8M (接近官方的700M)
训练显存需求
参数(FP16): 830.8M × 2B = 1661.6MB
优化器状态(FP32): 830.8M × 4B × 2 = 6646.4MB
梯度(FP16): 830.8M × 2B = 1661.6MB
激活值:
序列长度: 4096
批量大小: 2
估计激活值: ≈ 4-6GB
OLMo-700M训练总显存: 约14-16GB