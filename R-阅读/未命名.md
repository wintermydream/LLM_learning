### ​**​NTK（神经正切核）机制在本文中的作用​**​

本文的核心发现是：​**​参数初始化尺度（initialization scale）决定了Transformer在组合任务上是倾向于“推理”还是“记忆”​**​。而这一现象的背后，与​**​神经正切核（Neural Tangent Kernel, NTK）​**​的理论密切相关。

---

## ​**​1. NTK 的基本概念​**​

NTK 是深度学习理论中的一个重要工具，它描述了​**​无限宽神经网络（infinitely wide neural networks）​**​在训练初期的动态行为。简单来说：

- ​**​NTK 的核心思想​**​：在​**​大初始化（small γ）​**​时，神经网络的参数变化很小（接近线性动力学），因此模型的训练行为可以用一个固定的核函数（NTK）来描述。
- ​**​小初始化（large γ）​**​时，模型的参数变化较大，NTK 的假设不再严格成立，模型会进入​**​非线性动力学（nonlinear dynamics）​**​，导致更复杂的优化轨迹。