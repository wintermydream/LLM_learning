
"smart_sources:LLM/SFT/炼石成丹：大语言模型微调实战系列（二）模型微调篇.md": {"path":"LLM/SFT/炼石成丹：大语言模型微调实战系列（二）模型微调篇.md","last_embed":{"hash":null},"embeddings":{},"last_read":{"hash":"fdf8e8470bfa942232b8327e31c035892f61c5f353cace5df630cbe1053a276b","at":1746498778589},"class_name":"SmartSource","outlinks":[{"title":"LLaMA-Factory","target":"https://github.com/hiyouga/LLaMA-Factory","line":11},{"title":"FastChat","target":"https://github.com/lm-sys/FastChat","line":11},{"title":"DeepSpeedExamples","target":"https://github.com/microsoft/DeepSpeedExamples","line":11},{"title":"ms-swift","target":"https://github.com/modelscope/ms-swift","line":11},{"title":"unsloth","target":"https://github.com/unslothai/unsloth","line":11},{"title":"Firefly","target":"https://github.com/yangjianxin1/Firefly","line":11},{"title":"![","target":"https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practical-series-on-fine-tuning-large-language-models-part-two1.jpg","line":13},{"title":"开源代码","target":"https://github.com/aws-samples/llm_model_hub","line":17},{"title":"![","target":"https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practical-series-on-fine-tuning-large-language-models-part-two2.jpg","line":23},{"title":"![","target":"https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practical-series-on-fine-tuning-large-language-models-part-two3.jpg","line":73},{"title":"查看文档","target":"https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/algos.html","line":83},{"title":"参照链接","target":"https://github.com/aws/deep-learning-containers/blob/master/available_images.md","line":83},{"title":"扩展预构建容器","target":"https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/prebuilt-containers-extend.html","line":85},{"title":"![","target":"https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practical-series-on-fine-tuning-large-language-models-part-two4.jpg","line":95},{"title":"https://mp.weixin.qq.com/s/PEl\\_-1CRAJvss5pMFcU31w","target":"https://mp.weixin.qq.com/s/PEl_-1CRAJvss5pMFcU31w","line":97},{"title":"基于 Amazon SageMaker 和 LLaMA-Factory 打造一站式无代码模型微调部署平台 Model Hub","target":"https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/","line":99},{"title":"![","target":"https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practical-series-on-fine-tuning-large-language-models-part-two5.jpg","line":296},{"title":"https://amzn-chn.feishu.cn/docx/QniUdr7FroxShfxeoPacLJKtnXf","target":"https://amzn-chn.feishu.cn/docx/QniUdr7FroxShfxeoPacLJKtnXf","line":326},{"title":"https://github.com/aws-samples/llm\\_model\\_hub/","target":"https://github.com/aws-samples/llm_model_hub/","line":327},{"title":"炼石成丹：大语言模型微调实战系列（一）数据准备篇","target":"https://aws.amazon.com/cn/blogs/china/practical-series-on-fine-tuning-large-language-models-part-one/","line":332},{"title":"炼石成丹：大语言模型微调实战系列（三）模型评估篇","target":"https://aws.amazon.com/cn/blogs/china/practical-series-on-fine-tuning-large-language-models-part-three/","line":334}],"blocks":{"#":[1,8],"###1\\. 模型框架的选择":[9,18],"###1\\. 模型框架的选择#{1}":[11,18],"###2\\. 选择模型微调方法":[19,66],"###2\\. 选择模型微调方法#{1}":[21,60],"###2\\. 选择模型微调方法#{2}":[61,61],"###2\\. 选择模型微调方法#{3}":[62,62],"###2\\. 选择模型微调方法#{4}":[63,64],"###2\\. 选择模型微调方法#{5}":[65,66],"###3\\. 模型微调所需要的算力":[67,74],"###3\\. 模型微调所需要的算力#{1}":[69,74],"###4\\. 模型微调":[75,335],"###4\\. 模型微调#4.1 如何使用 SageMaker 进行微调":[77,86],"###4\\. 模型微调#4.1 如何使用 SageMaker 进行微调#{1}":[79,86],"###4\\. 模型微调#4.2 如何使用 ModelHub 进行微调":[87,100],"###4\\. 模型微调#4.2 如何使用 ModelHub 进行微调#{1}":[89,100],"###4\\. 模型微调#4.3 模型微调中的参数":[101,335],"###4\\. 模型微调#4.3 模型微调中的参数#{1}":[103,255],"###4\\. 模型微调#4.3 模型微调中的参数#{2}":[256,256],"###4\\. 模型微调#4.3 模型微调中的参数#{3}":[257,271],"###4\\. 模型微调#4.3 模型微调中的参数#{4}":[272,272],"###4\\. 模型微调#4.3 模型微调中的参数#{5}":[273,335]},"last_import":{"mtime":1745905169307,"size":19067,"at":1746498778590,"hash":"fdf8e8470bfa942232b8327e31c035892f61c5f353cace5df630cbe1053a276b"}},