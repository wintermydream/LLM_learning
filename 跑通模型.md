# 跑通模型
### 选定基础模型

1. 选定训练的目标模型：OLMo；
2. 选择1b的模型基础架构
3. 获取方式
   * 直接下载：[https://github.com/allenai/OLMo](https://github.com/allenai/OLMo)，问题：容易丢失文件
   * pip安装，问题：文件太大网络不稳定
   * 直接从远程文件夹copy，解决问题
   * 安装成功

```python
pip uninstall ai2-olmo
rm -rf ~/.cache/pip  # 清除 pip 缓存
cd OLMo
# 安装速度慢，可以改用国内镜像源
pip install -e .[all] -i https://pypi.tuna.tsinghua.edu.cn/simple
```
### 跑通预训练模型
1. yaml训练设置：[[Pretraining参数设置预训练 id=b0058d96-a12c-4e6f-914f-cf7824960759]]

2. WanDB数据库：OLMo训练程序中调用了WanDB数据库，因此需要注册并登录该数据库。访问WanDB官网 (https://wandb.ai/site)，注册账号，获取key。，这里使用Google账号注册

3. 运行命令，注意选择GPU节点，注意后台挂起

```python
CUDA_VISIBLE_DEVICES=1 nohup torchrun --nproc_per_node=1 scripts/train.py configs/myTest/try250421.yaml > train.log 2>&1 &
```


### 跑通推理模型
1. 下载已训练好的权重

```python
# 安装依赖
pip install -U hf-transfer huggingface_hub

# 启用hf_transfer（Linux/macOS）
export HF_HUB_ENABLE_HF_TRANSFER=1
export HF_ENDPOINT=https://hf-mirror.com

# 下载模型（示例：OLMo-1B）
• huggingface-cli download --resume-download allenai/OLMo-1B --local-dir ./olmo-1b
```
下载好的本地文件结构

```python
olmo-1b/
├── config.json
├── model.safetensors
├── tokenizer.json
└── (其他必要文件)
```


##### **预训练阶段（Pre-training）​**
**预训练阶段​**​：`config.json`初始化模型结构，`tokenizer.json`处理原始数据，训练后的权重保存为`model.safetensors`。

​**​目标​**​：通过海量数据训练模型，学习通用的语言表示能力（如语法、语义、常识）。
​**​关键文件作用​**​：

* ​**​****config.json****​**定义模型结构（如12层Transformer、隐藏层维度2048、注意力头数16等）。决定训练的超参数（如学习率调度、dropout率），但部分超参数可能在训练脚本中单独指定。
* **tokenizer.json****​**将原始文本（如维基百科、书籍）分词为子词（subword）或单词，转化为模型可处理的token ID序列。例如，句子`"OLMo is powerful"` → `[1024, 237, 593]`（假设的token ID映射）。
* ​**​****model.safetensors****​**初始化为随机权重（预训练阶段开始时），训练过程中逐步更新权重，最终保存为训练好的文件。

​**​训练流程​**​：

1. ​**​数据预处理​**​：使用分词器（`tokenizer.json`）将原始文本转换为token序列，并分batch（如每批4096个token）。
2. **模型前向传播​**​：模型（按`config.json`结构初始化）计算token序列的预测结果（如预测掩码词或下一token）。
3. ​**​损失计算与反向传播​**​：计算预测与真实token的交叉熵损失，通过梯度下降更新`model.safetensors`中的权重。
4. **保存检查点​**​：定期保存训练中的权重（`model.safetensors`），避免训练中断丢失进度。



##### **推理阶段（Inference）​**
**推理阶段​**​：三者配合完成文本输入到输出的端到端处理（分词→模型计算→生成文本）

​**​目标​**​：利用预训练好的模型执行具体任务（如文本生成、问答）。
​**​关键文件作用​**​：

* ​**​****config.json****​**确保加载的模型结构与训练时一致，避免架构不匹配错误。\* ​**​**
* **tokenizer.json****​**将用户输入的文本（如`"Explain OLMo"`）编码为token ID，并将模型输出的token ID解码为人类可读文本。
*  ​**model.safetensors****​**提供训练好的权重，模型基于这些参数计算输出（如生成回答）。

​**​推理流程​**​：

1. ​**​输入处理​**​：分词器将输入文本转换为token ID序列，并添加特殊标记（如`<bos>`、`<eos>`）。
2. **模型计算​**​：加载的模型（`config.json`+`model.safetensors`）对输入序列进行自回归生成（逐token预测）。
3. **输出生成​**​：例如，输入`"Explain OLMo"` → 模型生成`"OLMo is a 1B-parameter language model..."`，分词器将其解码为文本。

